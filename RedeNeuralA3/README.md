<h1><strong> üéû Rede Neural - Avalia√ß√£o de Filmes</strong></h1>

<p>Esse √© um projeto de uma rede neural criada e treinada para realizar a 
tarefa classifica√ß√£o de filmes em "bom" ou "ruim".</p>

<h2>Entendendo a l√≥gica de uma Rede Neural</h2>
<p><Strong>Rede neural</Strong> √© um m√©todo de intelig√™ncia artificial que ensina computadores a processar dados de forma semelhante ao c√©rebro 
humano. √â um tipo de processo de machine learning, que usa <strong>n√≥s ou neur√¥nios</strong> interconectados em uma estrutura em camadas, semelhante ao c√©rebro humano.
Uma rede neural artificial (ou RNA) √© composta por v√°rios neur√¥nios, cujo funcionamento √© bastante simples. Geralmente, os neur√¥nios s√£o conectadas por canais de comunica√ß√£o 
que est√£o associados a determinado <strong>peso</strong>. As unidades fazem opera√ß√µes apenas sobre seus dados locais, que s√£o entradas recebidas pelas suas conex√µes.</p>

<p>As arquiteturas neurais s√£o tipicamente organizadas entre <strong>camadas</strong>, e cada camada est√° conectada com a camada anterior:</p>
<ul>
    <li>Camada de Entrada: onde os padr√µes s√£o apresentados √† rede;</li>
    <li>Camadas Intermedi√°rias ou Ocultas: onde √© feita a maior parte do processamento, atrav√©s das conex√µes ponderadas; podem ser consideradas como extratoras de caracter√≠sticas;</li>
    <li>Camada de Sa√≠da: onde o resultado final √© conclu√≠do e apresentado.</li>
</ul>

<h2><strong>Explicando a Rede Neural do c√≥digo</strong></h2>

<p>A rede neural presente neste projeto tem o int√∫ito √∫nico de avaliar filmes atrav√©s de notas. Durante o processamento, a rede neural determinar√° a qualidade de um filme 
    em "bom" ou "ruim", sendo "bom" uma nota final acima de 7 e "ruim" uma nota final abaixo de 7.</p>

<p>Como dito anteriormente, RNAs (rede neural artificial) possuem uma caracter√≠stica distinta em sua arquitetura. Sua disposi√ß√£o se da em forma de camadas de processamento. No c√≥digo, fizemos o uso 
de 3 camadas:</p>
<ul>
    <li>A primeira camada 'layers.Input(shape=(1,))' √© a camada de entrada. Ela espera um vetor de entrada de tamanho 1.</li>
    <li>A segunda camada 'layers.Dense(4, activation='relu')' √© a camada oculta da rede neural. Ela possui 4 neur√¥nios e utiliza a fun√ß√£o de ativa√ß√£o ReLU.</li>
    <li>A terceira e √∫ltima camada 'layers.Dense(1, activation='sigmoid')' √© a camada de sa√≠da da rede neural. Ela possui 1 neur√¥nio e utiliza a fun√ß√£o de ativa√ß√£o sigm√≥ide.</li>
</ul>

<p>Durante toda a fase inicial de prepara√ß√£o da rede, cada neur√¥nio √© respons√°vel por ajustar seus pesos atrav√©s de c√°lculos matem√°ticos. Ao realizar essa etapa, a rede garante maior acur√°cia e estabilidade durante o treinamento 
de dados, pois todos os neur√¥nios estar√£o calibrados e prontos para a chegadas dos dados de valida√ß√£o.</p>

<p>Todo o c√°lculo executado nos neur√¥nios presentes nas camadas envolve puramente <strong>√°lgebra linear</strong>, eles fazem opera√ß√µes de multiplica√ß√µes de pesos pelas entradas e vi√©s.
Por exemplo, nas camadas layers.Dense (intermedi√°ria e de sa√≠da), cada neur√¥nio est√° conectado aos neur√¥nios da camada anterior atrav√©s de um conjunto de pesos, formando uma matriz.
Durante a forwardpropagation e na backpropagation, opera√ß√£o de multiplica√ß√£o de matrizes e vetores s√£o feitas para calcular as sa√≠das de rede e ajustar os pesos dos neur√¥nios.
Todos os calculos de matrizes e vetores s√£o necess√°rios para preparar os neur√¥nios para o treinamento com os dados de teste, para evitar perdas deefici√™ncia e poss√≠veis erros.</p>

<p>Ap√≥s, √© realizado o treinamento do modelo da rede neural. Ali, a rede ajusta todos os pesos nas camadas atrav√©s do algoritmo de descida de gradiente, que √© feito com c√°lculos de derivadas e opera√ß√µes 
de algebra linear. Nesta etapa, os dados recebidos se dividem em dados de treinamento e dados de valida√ß√£o. Os dados de treinamento s√£o aplicados primeiro para calibrar os pesos para os dados de teste, √© nesse ponto que a rede neural 
ir√° se preparar e aprender a processar os tipos de dados que ser√£o enviados futuramente. Em adi√ß√£o, temos os dados de valida√ß√£o, que s√£o enviados a rede logo ap√≥s os dados de treinamento para ajudar a garantir a assertividade das respostas retornadas.</p>

<p>Por fim, a rede faz previs√µes com novos dados recebidos, realizando as mesmas opera√ß√µes de √°lgebra linear para calcular as ativa√ß√µes das camadas. E finalmente, tem o resultado final de cada avalia√ß√£o de filme.</p>

<h2><strong>Ajuste de pesos da camada intermedi√°ria</strong></h2>

<p>Durante a etapa de processamento da camada intermedi√°ria, cada neur√¥nio realiza combina√ß√µes dos pesos atrav√©s de uma multiplica√ß√£o de matriz (pesos) por vetor (entradas). 
    Na camada intermedi√°ria/oculta com ativa√ß√£o ReLU '(layers.Dense(4, activation='relu'))', a multiplica√ß√£o de matriz e vetor funciona dessa forma:</p>
<ol>
    <li>vamos supor que a entrada seja um vetor de tamanho 1 (layers.Input(shape=(1,)))</li>
    <li>a camada oculta possui 4 neur√¥nios, cada um com seu conjunto de pesos e um vi√©s associado</li>
    <li>os pesos de uma camada densa s√£o representados como uma matriz, onde cada linha representa os pesos de um neur√¥nio</li>
    <li>se tivermos uma entrada de dimens√£o (1,) e 4 neur√¥nios na camada oculta, teremos uma matriz de pesos com dimens√£o (1, 4)</li>
    <li>a forwardpropagation, a entrada √© multiplicada pelos pesos dessa camada e, em seguida, somados os vi√©ses</li>
</ol>
<p>Essa opera√ß√£o √© uma multiplica√ß√£o de matriz por vetor, onde o vetor de entrada √© multiplicado pela matriz da camada densa.</p>

<p>Quando a rede neural inicia, os vi√©ses, assim como os pesos, s√£o inicializados com valores aleat√≥rios pequenos. 
Ao longo do treinamento, assim como os pesos, os vi√©ses s√£o ajustados iterativamente pelo algoritmo de otimiza√ß√£o para 
minimizar a diferen√ßa entre as previs√µes do modelo e as labels verdadeiras dos dados de treinamento.
O <strong>vi√©s</strong> √© necess√°rio para melhorar a capacidade da rede neural de aprender e generalizar a partir dos dados, 
permitindo um ajuste mais flex√≠vel da fun√ß√£o de ativa√ß√£o e dos limites de decis√£o, e seu valor √© ajustado durante o treinamento da rede.</p>

<p>Se fossemos fazer a estrutura de c√°lculo na m√£o, ficaria da seguinte forma:</p>
<ul>
    <li>entrada = np.array([2])- os dados de entrada</li>
    <li>pesos_camada_oculta = np.random.rand(1, 4) - o peso da camada oculta com 4 neur√¥nios</li>
    <li>vieses_camada_oculta = np.random.rand(4) - vi√©s da camada oculta, um para cada neur√¥nio</li>
</ul>
<p>Neste ponto, "entrada" √© um vetor de dimens√£o 1. O "pesos_camada_oculta" representa os pesos da camada oculta, e "vieses_camada_oculta" s√£o os vieses associados a cada neur√¥nio.</p>

<p>O calculo de matriz por vetor (entrada pelo peso) + o vi√©s √© aprensentado com a seguinte estrutura:</p>
<ol>
    <li>saida_camada_oculta = np.dot(entrada, pesos_camada_oculta) + vieses_camada_oculta - multiplica√ß√£o de matriz por vetor (entrada pelos pesos) e adi√ß√£o dos vieses</li>
    <li>saida_camada_oculta_relu = np.maximum(0, saida_camada_oculta) - aplica√ß√£o da fun√ß√£o de ativa√ß√£o ReLU</li>
    <li>print("Sa√≠da da camada oculta (ReLU):", saida_camada_oculta_relu)</li>
</ol>
<p>Nessa estrutura, utilizo o numpy para facilitar nos c√°lculos. A fun√ß√£o np.dot() √© utilizada para realizar 
a multiplica√ß√£o de matriz por vetor, e a fun√ß√£o np.maximum() √© utilizada para aplicar a fun√ß√£o de ativa√ß√£o ReLU aos resultados da multiplica√ß√£o.</p>


<h2>Ativa√ß√£o ReLU</h2>

<p>Quando definimos as camadas, nos adicionados um par√¢metro adicional para aprimorar ainda mais a efici√™ncia da rede.
A fun√ß√£o de ativa√ß√£o ReLU (Rectified Linear Unit) √© uma fun√ß√£o usada especialmente em camadas intermedi√°rias (camadas ocultas). 
Ela √© simples, mas extremamente eficaz e resolve alguns problemas das fun√ß√µes de ativa√ß√£o mais antigas, como a fun√ß√£o sigmoide (que tamb√©m ta sendo usada no c√≥digo).</p>

<p>A fun√ß√£o ReLU se da pela fun√ß√£o:<p></p>
<ul>
    <li>F(x)=max(0,x)</li>
</ul>
<p>Ou seja, para qualquer valor de entrada X, a fun√ß√£o ReLU retorna X se X for maior que 0. Em s√≠ntese, ela "zera" todos 
os valores negativos e mant√©m os valores positivos como est√£o. Nessa rede neural, o X na fun√ß√£o ReLU √© 
definido pelo resultado da multiplica√ß√£o da matriz pelo vetor e a adi√ß√£o do vi√©s.</p>
